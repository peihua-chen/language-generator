{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_experiment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZJgOvpdW25u",
        "colab_type": "text"
      },
      "source": [
        "Code adapted from main.py and generate.py at https://github.com/pytorch/examples/blob/master/word_language_model/."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOuzHqVSF1FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding: utf-8\n",
        "import argparse\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# import torch.onnx\n",
        "import torch.nn.functional as F\n",
        "\n",
        "path = \"./drive/My Drive/wikitext-2\"\n",
        "\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBNT1qM7V9Zk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Load data\n",
        "###############################################################################\n",
        "\n",
        "import os\n",
        "from io import open\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self, path):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
        "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
        "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids\n",
        "\n",
        "corpus = Corpus(path)\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAheXXswV9hZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Build the model\n",
        "###############################################################################\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, has_mask=True):\n",
        "        if has_mask:\n",
        "            device = src.device\n",
        "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "                self.src_mask = mask\n",
        "        else:\n",
        "            self.src_mask = None\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return F.log_softmax(output, dim=-1)\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xfl36ZM3V9lT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "# Training code\n",
        "###############################################################################\n",
        "\n",
        "bptt = 35\n",
        "log_interval = 200\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            # if args.model == 'Transformer':\n",
        "            output = model(data)\n",
        "            output = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n",
        "\n",
        "def train():\n",
        "    # Turn on training mode which enables dropout.\n",
        "    model.train()\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
        "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
        "        model.zero_grad()\n",
        "        output = model(data)\n",
        "        output = output.view(-1, ntokens)\n",
        "        loss = criterion(output, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        for p in model.parameters():\n",
        "            p.data.add_(-lr, p.grad.data)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                epoch, batch, len(train_data) // bptt, lr,\n",
        "                elapsed * 1000 / log_interval, cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQOxfbowV9pl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0f7f2dc-6bec-4ecb-9f61-5828ec2dc05a"
      },
      "source": [
        "# Loop over epochs.\n",
        "lr = 5.0\n",
        "best_val_loss = None\n",
        "epochs = 6\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(\"wiki_transformer.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"wiki_transformer.pt\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   200/ 2983 batches | lr 5.00 | ms/batch 741.02 | loss  8.14 | ppl  3426.31\n",
            "| epoch   1 |   400/ 2983 batches | lr 5.00 | ms/batch 736.08 | loss  6.95 | ppl  1044.96\n",
            "| epoch   1 |   600/ 2983 batches | lr 5.00 | ms/batch 735.11 | loss  6.53 | ppl   685.61\n",
            "| epoch   1 |   800/ 2983 batches | lr 5.00 | ms/batch 737.20 | loss  6.36 | ppl   578.16\n",
            "| epoch   1 |  1000/ 2983 batches | lr 5.00 | ms/batch 736.21 | loss  6.26 | ppl   525.74\n",
            "| epoch   1 |  1200/ 2983 batches | lr 5.00 | ms/batch 736.23 | loss  6.23 | ppl   506.96\n",
            "| epoch   1 |  1400/ 2983 batches | lr 5.00 | ms/batch 736.70 | loss  6.18 | ppl   480.89\n",
            "| epoch   1 |  1600/ 2983 batches | lr 5.00 | ms/batch 736.53 | loss  6.19 | ppl   488.23\n",
            "| epoch   1 |  1800/ 2983 batches | lr 5.00 | ms/batch 737.69 | loss  6.06 | ppl   429.80\n",
            "| epoch   1 |  2000/ 2983 batches | lr 5.00 | ms/batch 742.18 | loss  6.07 | ppl   432.24\n",
            "| epoch   1 |  2200/ 2983 batches | lr 5.00 | ms/batch 742.72 | loss  5.97 | ppl   391.43\n",
            "| epoch   1 |  2400/ 2983 batches | lr 5.00 | ms/batch 751.38 | loss  6.00 | ppl   402.65\n",
            "| epoch   1 |  2600/ 2983 batches | lr 5.00 | ms/batch 756.21 | loss  5.99 | ppl   400.48\n",
            "| epoch   1 |  2800/ 2983 batches | lr 5.00 | ms/batch 775.24 | loss  5.91 | ppl   369.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 2305.65s | valid loss  5.87 | valid ppl   352.53\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 2983 batches | lr 5.00 | ms/batch 774.89 | loss  5.90 | ppl   365.30\n",
            "| epoch   2 |   400/ 2983 batches | lr 5.00 | ms/batch 793.57 | loss  5.86 | ppl   351.73\n",
            "| epoch   2 |   600/ 2983 batches | lr 5.00 | ms/batch 778.24 | loss  5.69 | ppl   297.23\n",
            "| epoch   2 |   800/ 2983 batches | lr 5.00 | ms/batch 782.14 | loss  5.71 | ppl   301.53\n",
            "| epoch   2 |  1000/ 2983 batches | lr 5.00 | ms/batch 782.44 | loss  5.68 | ppl   294.12\n",
            "| epoch   2 |  1200/ 2983 batches | lr 5.00 | ms/batch 790.57 | loss  5.71 | ppl   301.49\n",
            "| epoch   2 |  1400/ 2983 batches | lr 5.00 | ms/batch 792.09 | loss  5.73 | ppl   306.47\n",
            "| epoch   2 |  1600/ 2983 batches | lr 5.00 | ms/batch 791.42 | loss  5.76 | ppl   315.93\n",
            "| epoch   2 |  1800/ 2983 batches | lr 5.00 | ms/batch 795.38 | loss  5.66 | ppl   286.11\n",
            "| epoch   2 |  2000/ 2983 batches | lr 5.00 | ms/batch 794.51 | loss  5.70 | ppl   298.41\n",
            "| epoch   2 |  2200/ 2983 batches | lr 5.00 | ms/batch 792.91 | loss  5.60 | ppl   270.88\n",
            "| epoch   2 |  2400/ 2983 batches | lr 5.00 | ms/batch 794.03 | loss  5.64 | ppl   281.04\n",
            "| epoch   2 |  2600/ 2983 batches | lr 5.00 | ms/batch 795.44 | loss  5.65 | ppl   284.60\n",
            "| epoch   2 |  2800/ 2983 batches | lr 5.00 | ms/batch 793.94 | loss  5.59 | ppl   268.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 2444.26s | valid loss  5.73 | valid ppl   309.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2983 batches | lr 5.00 | ms/batch 798.66 | loss  5.62 | ppl   275.06\n",
            "| epoch   3 |   400/ 2983 batches | lr 5.00 | ms/batch 787.91 | loss  5.61 | ppl   272.30\n",
            "| epoch   3 |   600/ 2983 batches | lr 5.00 | ms/batch 789.49 | loss  5.44 | ppl   230.42\n",
            "| epoch   3 |   800/ 2983 batches | lr 5.00 | ms/batch 787.02 | loss  5.49 | ppl   241.74\n",
            "| epoch   3 |  1000/ 2983 batches | lr 5.00 | ms/batch 802.98 | loss  5.47 | ppl   237.77\n",
            "| epoch   3 |  1200/ 2983 batches | lr 5.00 | ms/batch 788.26 | loss  5.48 | ppl   240.20\n",
            "| epoch   3 |  1400/ 2983 batches | lr 5.00 | ms/batch 785.27 | loss  5.52 | ppl   250.19\n",
            "| epoch   3 |  1600/ 2983 batches | lr 5.00 | ms/batch 780.63 | loss  5.56 | ppl   260.71\n",
            "| epoch   3 |  1800/ 2983 batches | lr 5.00 | ms/batch 797.08 | loss  5.46 | ppl   234.33\n",
            "| epoch   3 |  2000/ 2983 batches | lr 5.00 | ms/batch 778.86 | loss  5.50 | ppl   244.51\n",
            "| epoch   3 |  2200/ 2983 batches | lr 5.00 | ms/batch 779.44 | loss  5.41 | ppl   223.10\n",
            "| epoch   3 |  2400/ 2983 batches | lr 5.00 | ms/batch 777.66 | loss  5.45 | ppl   233.08\n",
            "| epoch   3 |  2600/ 2983 batches | lr 5.00 | ms/batch 775.79 | loss  5.46 | ppl   236.12\n",
            "| epoch   3 |  2800/ 2983 batches | lr 5.00 | ms/batch 774.92 | loss  5.41 | ppl   223.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 2425.71s | valid loss  5.69 | valid ppl   294.77\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 2983 batches | lr 5.00 | ms/batch 778.98 | loss  5.45 | ppl   231.84\n",
            "| epoch   4 |   400/ 2983 batches | lr 5.00 | ms/batch 773.23 | loss  5.44 | ppl   229.46\n",
            "| epoch   4 |   600/ 2983 batches | lr 5.00 | ms/batch 771.85 | loss  5.27 | ppl   194.54\n",
            "| epoch   4 |   800/ 2983 batches | lr 5.00 | ms/batch 772.55 | loss  5.33 | ppl   206.29\n",
            "| epoch   4 |  1000/ 2983 batches | lr 5.00 | ms/batch 770.05 | loss  5.31 | ppl   201.38\n",
            "| epoch   4 |  1200/ 2983 batches | lr 5.00 | ms/batch 769.60 | loss  5.33 | ppl   207.34\n",
            "| epoch   4 |  1400/ 2983 batches | lr 5.00 | ms/batch 767.14 | loss  5.38 | ppl   216.76\n",
            "| epoch   4 |  1600/ 2983 batches | lr 5.00 | ms/batch 771.07 | loss  5.41 | ppl   224.08\n",
            "| epoch   4 |  1800/ 2983 batches | lr 5.00 | ms/batch 780.37 | loss  5.32 | ppl   203.73\n",
            "| epoch   4 |  2000/ 2983 batches | lr 5.00 | ms/batch 767.03 | loss  5.37 | ppl   214.43\n",
            "| epoch   4 |  2200/ 2983 batches | lr 5.00 | ms/batch 764.77 | loss  5.27 | ppl   193.70\n",
            "| epoch   4 |  2400/ 2983 batches | lr 5.00 | ms/batch 765.44 | loss  5.31 | ppl   201.93\n",
            "| epoch   4 |  2600/ 2983 batches | lr 5.00 | ms/batch 781.33 | loss  5.34 | ppl   207.91\n",
            "| epoch   4 |  2800/ 2983 batches | lr 5.00 | ms/batch 766.86 | loss  5.28 | ppl   195.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 2387.95s | valid loss  5.65 | valid ppl   284.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 2983 batches | lr 5.00 | ms/batch 765.28 | loss  5.32 | ppl   204.85\n",
            "| epoch   5 |   400/ 2983 batches | lr 5.00 | ms/batch 760.95 | loss  5.32 | ppl   205.35\n",
            "| epoch   5 |   600/ 2983 batches | lr 5.00 | ms/batch 761.92 | loss  5.16 | ppl   174.50\n",
            "| epoch   5 |   800/ 2983 batches | lr 5.00 | ms/batch 759.23 | loss  5.21 | ppl   183.38\n",
            "| epoch   5 |  1000/ 2983 batches | lr 5.00 | ms/batch 759.63 | loss  5.21 | ppl   183.80\n",
            "| epoch   5 |  1200/ 2983 batches | lr 5.00 | ms/batch 759.09 | loss  5.24 | ppl   189.46\n",
            "| epoch   5 |  1400/ 2983 batches | lr 5.00 | ms/batch 760.03 | loss  5.27 | ppl   195.12\n",
            "| epoch   5 |  1600/ 2983 batches | lr 5.00 | ms/batch 760.09 | loss  5.31 | ppl   202.70\n",
            "| epoch   5 |  1800/ 2983 batches | lr 5.00 | ms/batch 758.55 | loss  5.22 | ppl   185.26\n",
            "| epoch   5 |  2000/ 2983 batches | lr 5.00 | ms/batch 756.84 | loss  5.26 | ppl   192.84\n",
            "| epoch   5 |  2200/ 2983 batches | lr 5.00 | ms/batch 756.98 | loss  5.16 | ppl   174.24\n",
            "| epoch   5 |  2400/ 2983 batches | lr 5.00 | ms/batch 756.35 | loss  5.22 | ppl   184.32\n",
            "| epoch   5 |  2600/ 2983 batches | lr 5.00 | ms/batch 754.99 | loss  5.24 | ppl   188.33\n",
            "| epoch   5 |  2800/ 2983 batches | lr 5.00 | ms/batch 752.69 | loss  5.18 | ppl   177.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 2347.29s | valid loss  5.62 | valid ppl   274.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 2983 batches | lr 5.00 | ms/batch 768.44 | loss  5.22 | ppl   185.65\n",
            "| epoch   6 |   400/ 2983 batches | lr 5.00 | ms/batch 759.81 | loss  5.23 | ppl   187.60\n",
            "| epoch   6 |   600/ 2983 batches | lr 5.00 | ms/batch 769.84 | loss  5.07 | ppl   159.20\n",
            "| epoch   6 |   800/ 2983 batches | lr 5.00 | ms/batch 754.26 | loss  5.12 | ppl   167.99\n",
            "| epoch   6 |  1000/ 2983 batches | lr 5.00 | ms/batch 756.10 | loss  5.12 | ppl   166.97\n",
            "| epoch   6 |  1200/ 2983 batches | lr 5.00 | ms/batch 756.39 | loss  5.14 | ppl   171.50\n",
            "| epoch   6 |  1400/ 2983 batches | lr 5.00 | ms/batch 773.52 | loss  5.20 | ppl   180.60\n",
            "| epoch   6 |  1600/ 2983 batches | lr 5.00 | ms/batch 755.39 | loss  5.23 | ppl   186.19\n",
            "| epoch   6 |  1800/ 2983 batches | lr 5.00 | ms/batch 754.57 | loss  5.14 | ppl   171.18\n",
            "| epoch   6 |  2000/ 2983 batches | lr 5.00 | ms/batch 755.64 | loss  5.19 | ppl   178.89\n",
            "| epoch   6 |  2200/ 2983 batches | lr 5.00 | ms/batch 755.02 | loss  5.07 | ppl   159.73\n",
            "| epoch   6 |  2400/ 2983 batches | lr 5.00 | ms/batch 754.37 | loss  5.13 | ppl   168.94\n",
            "| epoch   6 |  2600/ 2983 batches | lr 5.00 | ms/batch 752.78 | loss  5.16 | ppl   174.64\n",
            "| epoch   6 |  2800/ 2983 batches | lr 5.00 | ms/batch 752.96 | loss  5.10 | ppl   164.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 2344.63s | valid loss  5.64 | valid ppl   282.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.52 | test ppl   249.51\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHZsJY8OV92C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "31aa7be0-53c4-4f49-e2ea-9502f3fd44c7"
      },
      "source": [
        "###############################################################################\n",
        "# Generate Text\n",
        "###############################################################################\n",
        "temperature = 1.0\n",
        "nwords = 1000\n",
        "\n",
        "def generate_text(model, temperature, nwords, out):\n",
        "    model.eval()\n",
        "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "    \n",
        "    with open(out, 'w') as outf:\n",
        "        if nwords >= 1026583: outf.write(train_small + \"\\n\")\n",
        "        with torch.no_grad():  # no tracking history\n",
        "            for i in range(nwords):\n",
        "                # if is_transformer_model:\n",
        "                output = model(input, False)\n",
        "                word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                input = torch.cat([input, word_tensor], 0)\n",
        "\n",
        "                word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "                outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "                if i % log_interval == 0:\n",
        "                    print('| Generated {}/{} words'.format(i, nwords))\n",
        "\n",
        "generate_text(model, temperature, nwords, \"generated_whole.txt\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 800/1000 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyn-nET1Bszp",
        "colab_type": "text"
      },
      "source": [
        "Experiment: Augment Data with Generated Text\n",
        "-------------------------------------\n",
        "\n",
        "Train a model using only the first half of the original training set (300 articles, 1 million words), and select the best model as before. Use the best model to generate half the training set's worth of data, and run a model on a training set of half the original and half generated text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vLFscYr3qs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_str = \"\"\n",
        "with open(\"./drive/My Drive/wikitext-2/wiki.train.tokens\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    for line in f:\n",
        "        train_str += line\n",
        "\n",
        "val_str = \"\"\n",
        "with open(\"./drive/My Drive/wikitext-2/wiki.valid.tokens\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    for line in f:\n",
        "        val_str += line\n",
        "        \n",
        "test_str = \"\"\n",
        "with open(\"./drive/My Drive/wikitext-2/wiki.test.tokens\", encoding=\"utf-8\", errors='ignore') as f:\n",
        "    for line in f:\n",
        "        test_str += line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HZ-9Pw8CEi8",
        "colab_type": "text"
      },
      "source": [
        "Check that the halved dataset contains about half the articles, words, and chars. The same applies for val and test, though those numbers are not shown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTXxoMhMB5bK",
        "colab_type": "code",
        "outputId": "2ff1f74c-9671-42a9-dfb2-158919ce505f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train_small = train_str[:int(len(train_str) / 2) - 38]\n",
        "val_small = val_str[:int(len(val_str) / 2) + 1422]\n",
        "test_small = test_str[:int(len(test_str) / 2) + 869]\n",
        "\n",
        "with open(\"./drive/My Drive/wikitext-2/half_wiki.train.tokens\", \"a\") as f:\n",
        "    f.write(train_small)\n",
        "with open(\"./drive/My Drive/wikitext-2/half_wiki.val.tokens\", \"a\") as f:\n",
        "    f.write(val_small)\n",
        "with open(\"./drive/My Drive/wikitext-2/half_wiki.test.tokens\", \"a\") as f:\n",
        "    f.write(test_small)\n",
        "\n",
        "print(\"Number of article-like units in training: \", len(train_str.split(\"\\n \\n \\n\")))\n",
        "print(\"Number of article-like units in halved training: \", len(train_small.split(\"\\n \\n \\n\")))\n",
        "\n",
        "print(\"Number of word-like units in training: \", len(train_str.split()))\n",
        "print(\"Number of word-like units in halved training: \", len(train_small.split()))\n",
        "\n",
        "print(\"Number of chars units in training: \", len(train_str))\n",
        "print(\"Number of chars units in halved training: \", len(train_small))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of article-like units in training:  1191\n",
            "Number of article-like units in halved training:  616\n",
            "Number of word-like units in training:  2051910\n",
            "Number of word-like units in halved training:  1026583\n",
            "Number of chars units in training:  10780437\n",
            "Number of chars units in halved training:  5390180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fb6hGQBJXY4L"
      },
      "source": [
        "Train model with half training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhClTnM2XfCH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b46f20be-2c08-4117-8f99-dd041c17e75e"
      },
      "source": [
        "class Corpus(object):\n",
        "    def __init__(self, path, train_name, val_name, test_name):\n",
        "        self.dictionary = Dictionary()\n",
        "        self.train = self.tokenize(path + train_name)\n",
        "        self.valid = self.tokenize(path + val_name)\n",
        "        self.test = self.tokenize(path + test_name)\n",
        "\n",
        "    def tokenize(self, path):\n",
        "        \"\"\"Tokenizes a text file.\"\"\"\n",
        "        assert os.path.exists(path)\n",
        "        # Add words to the dictionary\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                for word in words:\n",
        "                    self.dictionary.add_word(word)\n",
        "\n",
        "        # Tokenize file content\n",
        "        with open(path, 'r', encoding=\"utf8\") as f:\n",
        "            idss = []\n",
        "            for line in f:\n",
        "                words = line.split() + ['<eos>']\n",
        "                ids = []\n",
        "                for word in words:\n",
        "                    ids.append(self.dictionary.word2idx[word])\n",
        "                idss.append(torch.tensor(ids).type(torch.int64))\n",
        "            ids = torch.cat(idss)\n",
        "\n",
        "        return ids\n",
        "        \n",
        "corpus = Corpus(path, \"half_wiki.train.tokens\", \"half_wiki.val.tokens\", \"half_wiki.test.tokens\")\n",
        "\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "lr = 1.25\n",
        "best_val_loss = None\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(\"wiki_transformer_half.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"wiki_transformer_half.pt\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./drive/My Drive/wikitext-2/half_wiki.train.tokens\n",
            "./drive/My Drive/wikitext-2/half_wiki.val.tokens\n",
            "./drive/My Drive/wikitext-2/half_wiki.test.tokens\n",
            "| epoch   1 |   200/ 1492 batches | lr 1.25 | ms/batch 652.82 | loss  7.52 | ppl  1847.52\n",
            "| epoch   1 |   400/ 1492 batches | lr 1.25 | ms/batch 651.70 | loss  6.82 | ppl   913.93\n",
            "| epoch   1 |   600/ 1492 batches | lr 1.25 | ms/batch 668.99 | loss  6.58 | ppl   722.11\n",
            "| epoch   1 |   800/ 1492 batches | lr 1.25 | ms/batch 648.97 | loss  6.49 | ppl   659.00\n",
            "| epoch   1 |  1000/ 1492 batches | lr 1.25 | ms/batch 647.90 | loss  6.41 | ppl   610.48\n",
            "| epoch   1 |  1200/ 1492 batches | lr 1.25 | ms/batch 647.53 | loss  6.34 | ppl   567.52\n",
            "| epoch   1 |  1400/ 1492 batches | lr 1.25 | ms/batch 651.65 | loss  6.24 | ppl   511.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 1011.22s | valid loss  6.06 | valid ppl   429.34\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   2 |   200/ 1492 batches | lr 1.25 | ms/batch 655.15 | loss  6.15 | ppl   469.77\n",
            "| epoch   2 |   400/ 1492 batches | lr 1.25 | ms/batch 650.35 | loss  6.04 | ppl   421.61\n",
            "| epoch   2 |   600/ 1492 batches | lr 1.25 | ms/batch 651.47 | loss  5.95 | ppl   382.84\n",
            "| epoch   2 |   800/ 1492 batches | lr 1.25 | ms/batch 651.32 | loss  5.93 | ppl   375.08\n",
            "| epoch   2 |  1000/ 1492 batches | lr 1.25 | ms/batch 649.93 | loss  5.91 | ppl   368.27\n",
            "| epoch   2 |  1200/ 1492 batches | lr 1.25 | ms/batch 650.74 | loss  5.93 | ppl   374.48\n",
            "| epoch   2 |  1400/ 1492 batches | lr 1.25 | ms/batch 650.03 | loss  5.85 | ppl   345.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 1008.40s | valid loss  5.81 | valid ppl   335.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 1492 batches | lr 1.25 | ms/batch 652.59 | loss  5.80 | ppl   329.12\n",
            "| epoch   3 |   400/ 1492 batches | lr 1.25 | ms/batch 647.57 | loss  5.75 | ppl   312.94\n",
            "| epoch   3 |   600/ 1492 batches | lr 1.25 | ms/batch 647.91 | loss  5.66 | ppl   286.66\n",
            "| epoch   3 |   800/ 1492 batches | lr 1.25 | ms/batch 648.24 | loss  5.64 | ppl   280.98\n",
            "| epoch   3 |  1000/ 1492 batches | lr 1.25 | ms/batch 648.16 | loss  5.64 | ppl   281.02\n",
            "| epoch   3 |  1200/ 1492 batches | lr 1.25 | ms/batch 650.71 | loss  5.68 | ppl   293.73\n",
            "| epoch   3 |  1400/ 1492 batches | lr 1.25 | ms/batch 649.71 | loss  5.61 | ppl   273.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 1006.10s | valid loss  5.69 | valid ppl   294.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/ 1492 batches | lr 1.25 | ms/batch 653.87 | loss  5.56 | ppl   259.13\n",
            "| epoch   4 |   400/ 1492 batches | lr 1.25 | ms/batch 649.47 | loss  5.54 | ppl   253.79\n",
            "| epoch   4 |   600/ 1492 batches | lr 1.25 | ms/batch 665.23 | loss  5.45 | ppl   232.07\n",
            "| epoch   4 |   800/ 1492 batches | lr 1.25 | ms/batch 652.27 | loss  5.43 | ppl   229.07\n",
            "| epoch   4 |  1000/ 1492 batches | lr 1.25 | ms/batch 652.45 | loss  5.44 | ppl   230.25\n",
            "| epoch   4 |  1200/ 1492 batches | lr 1.25 | ms/batch 649.53 | loss  5.49 | ppl   242.77\n",
            "| epoch   4 |  1400/ 1492 batches | lr 1.25 | ms/batch 650.50 | loss  5.42 | ppl   226.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 1015.16s | valid loss  5.61 | valid ppl   273.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/ 1492 batches | lr 1.25 | ms/batch 652.21 | loss  5.37 | ppl   213.96\n",
            "| epoch   5 |   400/ 1492 batches | lr 1.25 | ms/batch 649.62 | loss  5.37 | ppl   214.59\n",
            "| epoch   5 |   600/ 1492 batches | lr 1.25 | ms/batch 648.73 | loss  5.28 | ppl   195.44\n",
            "| epoch   5 |   800/ 1492 batches | lr 1.25 | ms/batch 647.70 | loss  5.27 | ppl   193.69\n",
            "| epoch   5 |  1000/ 1492 batches | lr 1.25 | ms/batch 648.23 | loss  5.27 | ppl   194.58\n",
            "| epoch   5 |  1200/ 1492 batches | lr 1.25 | ms/batch 647.51 | loss  5.34 | ppl   208.61\n",
            "| epoch   5 |  1400/ 1492 batches | lr 1.25 | ms/batch 646.76 | loss  5.27 | ppl   193.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 1004.67s | valid loss  5.57 | valid ppl   262.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/ 1492 batches | lr 1.25 | ms/batch 652.50 | loss  5.21 | ppl   182.73\n",
            "| epoch   6 |   400/ 1492 batches | lr 1.25 | ms/batch 647.86 | loss  5.23 | ppl   186.09\n",
            "| epoch   6 |   600/ 1492 batches | lr 1.25 | ms/batch 646.29 | loss  5.13 | ppl   168.61\n",
            "| epoch   6 |   800/ 1492 batches | lr 1.25 | ms/batch 645.98 | loss  5.12 | ppl   167.60\n",
            "| epoch   6 |  1000/ 1492 batches | lr 1.25 | ms/batch 645.73 | loss  5.13 | ppl   169.09\n",
            "| epoch   6 |  1200/ 1492 batches | lr 1.25 | ms/batch 649.48 | loss  5.21 | ppl   182.27\n",
            "| epoch   6 |  1400/ 1492 batches | lr 1.25 | ms/batch 649.30 | loss  5.13 | ppl   169.43\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 1004.18s | valid loss  5.53 | valid ppl   252.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  5.46 | test ppl   234.05\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCCxiS4FdWmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "48a9aa2c-f681-41e9-9b4f-2991867f1765"
      },
      "source": [
        "nwords = 1026583\n",
        "log_interval = 10000\n",
        "\n",
        "def generate_text(model, temperature, nwords, out):\n",
        "    model.eval()\n",
        "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "    \n",
        "    text = \"\"\n",
        "    with torch.no_grad():  # no tracking history\n",
        "        for i in range(nwords):\n",
        "            output = model(input, False)\n",
        "            word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "            input = torch.cat([input, word_tensor], 0)\n",
        "\n",
        "            word = corpus.dictionary.idx2word[word_idx]\n",
        "            text += word + ' '\n",
        "\n",
        "            if i % log_interval == 0:\n",
        "                print('| Generated {}/{} words'.format(i, nwords))\n",
        "    \n",
        "    with open(out, 'w') as f:\n",
        "        if nwords >= 1026583: f.write(train_small + \"\\n\")\n",
        "        f.write(text)\n",
        "\n",
        "generate_text(model, temperature, nwords, \"./drive/My Drive/wikitext-2/generated_half_train.txt\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/1026583 words\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-3b0c96b97946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./drive/My Drive/wikitext-2/generated_half_train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-3b0c96b97946>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, temperature, nwords, out)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# no tracking history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mword_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mword_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6b7606ee692c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, has_mask)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTafPRdg_0dp",
        "colab_type": "text"
      },
      "source": [
        "After 3 hours, I stopped this cell. Unfortunately, I don't know how to speed up the text generation, so I was unable to test my data augmentation idea."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HSjh5QgXeo2",
        "colab_type": "text"
      },
      "source": [
        "Run another model with half original dataset, half generated text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzlZaiZyyTM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = Corpus(path, \"generated_half_train.txt\", \"wiki.valid.tokens\", \"wiki.test.tokens\")\n",
        "\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "lr = 1.25\n",
        "best_val_loss = None\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(\"wiki_transformer_half.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"wiki_transformer_half.pt\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oZgN6Sh0xOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nwords = 1000\n",
        "log_interval = 200\n",
        "generate_text(model, temperature, nwords, \"./drive/My Drive/wikitext-2/gen_half.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85MhN8wkox5m",
        "colab_type": "text"
      },
      "source": [
        "Since the model trained on only half the training data generated similar results to the model utilizing the full dataset, we run the experiment again with even less training data.\n",
        "\n",
        "This time, we try using only the first 5% of the original training set (30 articles, 100,000 words -- about twice the length of *The Great Gatsby*). Using the first fifth does not eliminate the possibility that maybe the first half of the training set is just particularly informative by chance and the success is not due to the transformer architecture, but it cuts that probability in half. If this model performs poorly, we may guess that the model did not have enough training data to succeed. By keeping to the first half, it we avoid confounding our conclusion with the possibility that the second half of the training data was uninformative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT-TD0VvoVpN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_small = train_str[:int(len(test_str) / 20) - 229]\n",
        "val_small = val_str[:int(len(train_str)/20)+71]\n",
        "test_small = test_str[:int(len(val_str)/20)+2575]\n",
        "\n",
        "with open(\"./drive/My Drive/wikitext-2/five_wiki.train.tokens\", \"a\") as f:\n",
        "    f.write(train_small)\n",
        "with open(\"./drive/My Drive/wikitext-2/five_wiki.val.tokens\", \"a\") as f:\n",
        "    f.write(val_small)\n",
        "with open(\"./drive/My Drive/wikitext-2/five_wiki.test.tokens\", \"a\") as f:\n",
        "    f.write(test_small)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgQpnj8rzD20",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "44257e6e-ec0d-4e41-ba6c-75ea03162d8c"
      },
      "source": [
        "corpus = Corpus(path, \"five_wiki.train.tokens\", \"five_wiki.val.tokens\", \"five_wiki.test.tokens\")\n",
        "\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "lr = 2\n",
        "best_val_loss = None\n",
        "epochs = 10\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(\"wiki_transformer_half.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"wiki_transformer_half.pt\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 23.76s | valid loss  7.85 | valid ppl  2573.33\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 23.72s | valid loss  7.12 | valid ppl  1240.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 23.73s | valid loss  7.20 | valid ppl  1339.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time: 23.95s | valid loss  6.81 | valid ppl   905.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time: 23.85s | valid loss  6.80 | valid ppl   896.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time: 23.73s | valid loss  6.79 | valid ppl   887.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time: 23.71s | valid loss  6.79 | valid ppl   886.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time: 24.14s | valid loss  6.79 | valid ppl   891.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time: 23.80s | valid loss  6.80 | valid ppl   897.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time: 23.96s | valid loss  6.80 | valid ppl   898.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "=========================================================================================\n",
            "| End of training | test loss  6.85 | test ppl   942.18\n",
            "=========================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxoSxcpQzY_7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "d3d6929c-686e-403d-d37c-3ea912e99de9"
      },
      "source": [
        "temperature = 1.0\n",
        "nwords = 1000\n",
        "log_interval = 100\n",
        "def generate_text(model, temperature, nwords, out):\n",
        "    model.eval()\n",
        "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
        "    \n",
        "    with open(out, 'w') as outf:\n",
        "        if nwords >= 1026583: outf.write(train_small + \"\\n\")\n",
        "        with torch.no_grad():  # no tracking history\n",
        "            for i in range(nwords):\n",
        "                # if is_transformer_model:\n",
        "                output = model(input, False)\n",
        "                word_weights = output[-1].squeeze().div(temperature).exp().cpu()\n",
        "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "                word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
        "                input = torch.cat([input, word_tensor], 0)\n",
        "\n",
        "                word = corpus.dictionary.idx2word[word_idx]\n",
        "\n",
        "                outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
        "\n",
        "                if i % log_interval == 0:\n",
        "                    print('| Generated {}/{} words'.format(i, nwords))\n",
        "generate_text(model, temperature, nwords, \"./drive/My Drive/wikitext-2/generated_five_train.txt\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Generated 0/1000 words\n",
            "| Generated 100/1000 words\n",
            "| Generated 200/1000 words\n",
            "| Generated 300/1000 words\n",
            "| Generated 400/1000 words\n",
            "| Generated 500/1000 words\n",
            "| Generated 600/1000 words\n",
            "| Generated 700/1000 words\n",
            "| Generated 800/1000 words\n",
            "| Generated 900/1000 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTi00h7Y0Q1l",
        "colab_type": "text"
      },
      "source": [
        "Run model trained on 5% original training data, 95% generated text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL1_idX70GA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = Corpus(path, \"generated_five_train.txt\", \"wiki.valid.tokens\", \"wiki.test.tokens\")\n",
        "\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.valid, eval_batch_size)\n",
        "test_data = batchify(corpus.test, eval_batch_size)\n",
        "\n",
        "ntokens = len(corpus.dictionary)\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
        "lr = 1.25\n",
        "best_val_loss = None\n",
        "\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        train()\n",
        "        val_loss = evaluate(val_data)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                           val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "        # Save the model if the validation loss is the best we've seen so far.\n",
        "        if not best_val_loss or val_loss < best_val_loss:\n",
        "            with open(\"wiki_transformer_half.pt\", 'wb') as f:\n",
        "                torch.save(model, f)\n",
        "            best_val_loss = val_loss\n",
        "        else:\n",
        "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
        "            lr /= 4.0\n",
        "except KeyboardInterrupt:\n",
        "    print('-' * 89)\n",
        "    print('Exiting from training early')\n",
        "\n",
        "# Load the best saved model.\n",
        "with open(\"wiki_transformer_half.pt\", 'rb') as f:\n",
        "    model = torch.load(f)\n",
        "\n",
        "# Run on test data.\n",
        "test_loss = evaluate(test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f10-Udbs0ZE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nwords = 1000\n",
        "log_interval = 200\n",
        "generate_text(model, temperature, nwords, \"./drive/My Drive/wikitext-2/gen_five.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgjAXaJy1FcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}